# langgraph_poc.py

import streamlit as st
import pandas as pd
from docx import Document
import json
import base64
import re
import requests
from langgraph.graph import StateGraph, END
from typing import TypedDict
from pyvis.network import Network
import tempfile
import os
import streamlit.components.v1 as components

# =======================
# üîß LLM CONFIG
# =======================
LLM_API_URL = "https://your-llm-endpoint.com/v1/chat/completions"
LLM_USERNAME = "your_username_here"
LLM_PASSWORD = "your_password_here"

def get_basic_auth():
    creds = f"{LLM_USERNAME}:{LLM_PASSWORD}"
    return base64.b64encode(creds.encode()).decode()

# =======================
# üìÑ TEXT EXTRACTORS
# =======================
def extract_text_from_file(file):
    try:
        if file.name.endswith(".xlsx"):
            df = pd.read_excel(file)
            return df.to_csv(index=False)
        elif file.name.endswith(".docx"):
            doc = Document(file)
            return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        elif file.name.endswith(".csv"):
            df = pd.read_csv(file)
            return df.to_csv(index=False)
    except Exception as e:
        return f"Error reading file: {e}"
    return ""

# =======================
# ü§ñ LLM Functions
# =======================
def llm_extract_graph(text):
    if LLM_USERNAME == "your_username_here":
        return {
            "nodes": [{"id": "Web App", "type": "Application"}, {"id": "Database", "type": "Database"}],
            "edges": [{"source": "Web App", "target": "Database", "type": "USES"}]
        }
    
    prompt = f"""Extract entities and relationships from this document. Return JSON only:
{{"nodes": [{{"id": "name", "type": "Application|Database|Component"}}], "edges": [{{"source": "A", "target": "B", "type": "USES|RUNS_ON"}}]}}

Document: {text[:2000]}"""
    
    try:
        headers = {"Authorization": f"Basic {get_basic_auth()}", "Content-Type": "application/json"}
        
        # Try different payload formats (including Llama-specific)
        payload_options = [
            # Format 1: Llama/Ollama format
            {"model": "llama", "prompt": prompt, "stream": False, "options": {"temperature": 0.3, "num_predict": 600}},
            
            # Format 2: Standard Llama API
            {"inputs": prompt, "parameters": {"temperature": 0.3, "max_new_tokens": 600}},
            
            # Format 3: OpenAI-compatible format
            {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": prompt}], "temperature": 0.3, "max_tokens": 600},
            
            # Format 4: Simple Llama format
            {"prompt": prompt, "temperature": 0.3, "max_tokens": 600},
            
            # Format 5: Hugging Face format
            {"inputs": prompt, "options": {"wait_for_model": True}, "parameters": {"temperature": 0.3, "max_length": 600}}
        ]
        
        for i, payload in enumerate(payload_options):
            response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                # Try different response formats
                try:
                    resp_json = response.json()
                    
                    # Try different response formats (including Llama)
                    if "choices" in resp_json:
                        content = resp_json["choices"][0]["message"]["content"]
                    # Llama/Ollama response format
                    elif "response" in resp_json:
                        content = resp_json["response"]
                    # Direct text response
                    elif "text" in resp_json:
                        content = resp_json["text"]
                    # Generated text field
                    elif "generated_text" in resp_json:
                        content = resp_json["generated_text"]
                    # Hugging Face format
                    elif isinstance(resp_json, list) and len(resp_json) > 0:
                        content = resp_json[0].get("generated_text", str(resp_json))
                    else:
                        content = str(resp_json)
                    
                    st.write(f"**Success with payload format {i+1}**")
                    st.write("**LLM Response:**", content)
                    
                    # Enhanced JSON extraction for Llama responses
                    def extract_json_from_text(text):
                        # Clean the text
                        text = text.strip()
                        
                        # Try multiple JSON extraction methods
                        json_patterns = [
                            r'```json\s*(\{.*?\})\s*```',  # JSON in code blocks
                            r'```\s*(\{.*?\})\s*```',      # JSON in any code block
                            r'(\{[^{}]*"nodes"[^{}]*\})',   # Look for nodes pattern
                            r'(\{.*?"nodes".*?\})',         # Broader nodes pattern
                            r'(\{.*\})',                    # Any JSON-like structure
                        ]
                        
                        for pattern in json_patterns:
                            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
                            for match in matches:
                                try:
                                    # Clean up common issues
                                    cleaned = match.strip()
                                    # Fix trailing commas
                                    cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
                                    # Fix single quotes to double quotes
                                    cleaned = re.sub(r"'([^']*)':", r'"\1":', cleaned)
                                    
                                    result = json.loads(cleaned)
                                    if isinstance(result, dict) and ("nodes" in result or "edges" in result):
                                        return result
                                except:
                                    continue
                        
                        # If no JSON found, try to extract entities manually
                        st.write("**Attempting manual extraction...**")
                        
                        # Look for entity mentions
                        lines = text.split('\n')
                        nodes = []
                        edges = []
                        
                        for line in lines:
                            line = line.strip()
                            # Look for entity patterns
                            if any(word in line.lower() for word in ['application', 'database', 'component', 'system']):
                                # Extract potential entity name
                                words = line.split()
                                for word in words:
                                    if len(word) > 3 and word.isalnum():
                                        nodes.append({"id": word, "type": "Component"})
                                        if len(nodes) >= 3:  # Limit to prevent too many
                                            break
                        
                        # Create some basic relationships if we have nodes
                        if len(nodes) >= 2:
                            edges.append({"source": nodes[0]["id"], "target": nodes[1]["id"], "type": "USES"})
                        
                        return {"nodes": nodes[:5], "edges": edges[:3]} if nodes else None
                    
                    # Try to extract JSON
                    result = extract_json_from_text(content)
                    
                    if result:
                        st.write("**Extracted JSON:**", result)
                        return result
                    else:
                        st.error("Could not extract valid JSON from response")
                        st.write("**Raw response for debugging:**", content[:500])
                        return {"nodes": [], "edges": []}
                        
                except Exception as e:
                    st.error(f"Response parsing error: {e}")
                    continue
            else:
                st.write(f"**Payload format {i+1} failed:** {response.status_code} - {response.text}")
                continue
        
        st.error("All payload formats failed")
        return {"nodes": [], "edges": []}
            
    except Exception as e:
        st.error(f"LLM call failed: {e}")
        return {"nodes": [], "edges": []}

# =======================
# üîç LangGraph Setup
# =======================
class GraphState(TypedDict):
    file: object
    text: str
    structured_summary: str
    graph: dict

def extract_step(state: GraphState):
    text = extract_text_from_file(state["file"])
    st.write("**Extracted Text Length:**", len(text))
    
    # Better data structure analysis
    if text:
        lines = text.split('\n')
        if len(lines) > 1:
            # Likely CSV/Excel data
            headers = lines[0].split(',') if ',' in lines[0] else lines[0].split('\t')
            st.write("**Detected Headers:**", headers)
            
            # Show sample data rows
            sample_rows = []
            for i in range(1, min(4, len(lines))):
                if lines[i].strip():
                    row_data = lines[i].split(',') if ',' in lines[i] else lines[i].split('\t')
                    sample_rows.append(row_data)
            
            if sample_rows:
                st.write("**Sample Data Rows:**")
                for i, row in enumerate(sample_rows):
                    st.write(f"Row {i+1}: {row}")
            
            # Create structured data summary for better LLM understanding
            structured_summary = f"HEADERS: {headers}\n\nSAMPLE DATA:\n"
            for i, row in enumerate(sample_rows):
                structured_summary += f"Row {i+1}: {dict(zip(headers, row))}\n"
            
            return {"text": text, "structured_summary": structured_summary}
    
    return {"text": text, "structured_summary": ""}

def extract_kg_step(state: GraphState):
    return {"graph": llm_extract_graph(state["text"], state.get("structured_summary", ""))}

def build_graph_pipeline():
    workflow = StateGraph(GraphState)
    workflow.add_node("extract_text", extract_step)
    workflow.add_node("extract_kg", extract_kg_step)
    workflow.set_entry_point("extract_text")
    workflow.add_edge("extract_text", "extract_kg")
    workflow.add_edge("extract_kg", END)
    return workflow.compile()

# =======================
# üåê Visualization
# =======================
def show_graph(graph_data):
    try:
        net = Network(height="600px", width="100%", directed=True, bgcolor="#ffffff")
        
        color_map = {"Application": "#6C5CE7", "Database": "#00B894", "Component": "#FAB1A0"}
        
        for node in graph_data.get("nodes", []):
            color = color_map.get(node.get("type", ""), "#BDC3C7")
            net.add_node(node["id"], label=node["id"], color=color, size=25)

        for edge in graph_data.get("edges", []):
            net.add_edge(edge["source"], edge["target"], label=edge["type"])

        import uuid
        temp_file = f"temp_{uuid.uuid4().hex[:8]}.html"
        temp_path = os.path.join(tempfile.gettempdir(), temp_file)
        
        net.save_graph(temp_path)
        with open(temp_path, "r", encoding="utf-8") as f:
            html = f.read()
        components.html(html, height=650)
        
        try:
            os.unlink(temp_path)
        except:
            pass
            
    except Exception as e:
        st.error(f"Visualization error: {e}")

# =======================
# üñºÔ∏è Streamlit UI
# =======================
def main():
    st.title("üîó Knowledge Graph POC (with LangGraph)")
    
    llm_status = "üü¢ Active" if LLM_USERNAME != "your_username_here" else "üî¥ Demo Mode"
    st.write(f"**LLM Status:** {llm_status}")
    
    uploaded = st.file_uploader("üìÅ Upload Document", type=["xlsx", "docx", "csv"])

    if uploaded:
        with st.spinner("üîç Processing through LangGraph pipeline..."):
            graph_pipeline = build_graph_pipeline()
            output = graph_pipeline.invoke({"file": uploaded})
            kg = output["graph"]

        if kg.get("nodes"):
            st.success("‚úÖ Knowledge graph generated!")
            show_graph(kg)
            
            if kg.get("nodes"):
                node_ids = [n['id'] for n in kg["nodes"]]
                selected_node = st.selectbox("üîç Select node", node_ids)
                
                if selected_node:
                    node_info = next((n for n in kg["nodes"] if n["id"] == selected_node), None)
                    related_edges = [e for e in kg["edges"] if e["source"] == selected_node or e["target"] == selected_node]
                    
                    st.subheader(f"üìå {selected_node}")
                    if node_info:
                        st.write(f"**Type:** {node_info.get('type', 'Unknown')}")
                    
                    for rel in related_edges:
                        direction = "‚Üí" if rel["source"] == selected_node else "‚Üê"
                        other = rel["target"] if rel["source"] == selected_node else rel["source"]
                        st.write(f"‚Ä¢ {selected_node} {direction} **{rel['type']}** {direction} {other}")
            
            st.download_button("üì• Download JSON", json.dumps(kg, indent=2), file_name="kg.json")
        else:
            st.warning("No graph data extracted.")

if __name__ == "__main__":
    main()
