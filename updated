# langgraph_poc.py

import streamlit as st
import pandas as pd
from docx import Document
import json
import base64
import re
import requests
from langgraph.graph import StateGraph, END
from typing import TypedDict
from pyvis.network import Network
import tempfile
import os
import streamlit.components.v1 as components

# =======================
# üîß LLM CONFIG
# =======================
LLM_API_URL = "https://your-llm-endpoint.com/v1/chat/completions"
LLM_USERNAME = "your_username_here"
LLM_PASSWORD = "your_password_here"

def get_basic_auth():
    creds = f"{LLM_USERNAME}:{LLM_PASSWORD}"
    return base64.b64encode(creds.encode()).decode()

# =======================
# üìÑ TEXT EXTRACTORS
# =======================
def extract_text_from_file(file):
    try:
        if file.name.endswith(".xlsx"):
            df = pd.read_excel(file)
            return df.to_csv(index=False)
        elif file.name.endswith(".docx"):
            doc = Document(file)
            return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        elif file.name.endswith(".csv"):
            df = pd.read_csv(file)
            return df.to_csv(index=False)
    except Exception as e:
        return f"Error reading file: {e}"
    return ""

# =======================
# ü§ñ LLM Functions
# =======================
def llm_extract_graph(text):
    if LLM_USERNAME == "your_username_here":
        return {
            "nodes": [{"id": "Web App", "type": "Application"}, {"id": "Database", "type": "Database"}],
            "edges": [{"source": "Web App", "target": "Database", "type": "USES"}]
        }
    
    prompt = f"""Extract entities and relationships from this document. Return JSON only:
{{"nodes": [{{"id": "name", "type": "Application|Database|Component"}}], "edges": [{{"source": "A", "target": "B", "type": "USES|RUNS_ON"}}]}}

Document: {text[:2000]}"""
    
    try:
        headers = {"Authorization": f"Basic {get_basic_auth()}", "Content-Type": "application/json"}
        
        # Try different payload formats (including Llama-specific)
        payload_options = [
            # Format 1: Llama/Ollama format
            {"model": "llama", "prompt": prompt, "stream": False, "options": {"temperature": 0.3, "num_predict": 600}},
            
            # Format 2: Standard Llama API
            {"inputs": prompt, "parameters": {"temperature": 0.3, "max_new_tokens": 600}},
            
            # Format 3: OpenAI-compatible format
            {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": prompt}], "temperature": 0.3, "max_tokens": 600},
            
            # Format 4: Simple Llama format
            {"prompt": prompt, "temperature": 0.3, "max_tokens": 600},
            
            # Format 5: Hugging Face format
            {"inputs": prompt, "options": {"wait_for_model": True}, "parameters": {"temperature": 0.3, "max_length": 600}}
        ]
        
        for i, payload in enumerate(payload_options):
            response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                # Try different response formats
                try:
                    resp_json = response.json()
                    
                    # Try different response formats (including Llama)
                    if "choices" in resp_json:
                        content = resp_json["choices"][0]["message"]["content"]
                    # Llama/Ollama response format
                    elif "response" in resp_json:
                        content = resp_json["response"]
                    # Direct text response
                    elif "text" in resp_json:
                        content = resp_json["text"]
                    # Generated text field
                    elif "generated_text" in resp_json:
                        content = resp_json["generated_text"]
                    # Hugging Face format
                    elif isinstance(resp_json, list) and len(resp_json) > 0:
                        content = resp_json[0].get("generated_text", str(resp_json))
                    else:
                        content = str(resp_json)
                    
                    st.write(f"**Success with payload format {i+1}**")
                    st.write("**LLM Response:**", content)
                    
                    # Parse JSON from response
                    match = re.search(r'{.*}', content, re.DOTALL)
                    if match:
                        try:
                            result = json.loads(match.group())
                            st.write("**Parsed JSON:**", result)
                            return result
                        except json.JSONDecodeError as e:
                            st.error(f"JSON parsing error: {e}")
                            return {"nodes": [], "edges": []}
                    else:
                        st.error("No JSON found in response")
                        return {"nodes": [], "edges": []}
                        
                except Exception as e:
                    st.error(f"Response parsing error: {e}")
                    continue
            else:
                st.write(f"**Payload format {i+1} failed:** {response.status_code} - {response.text}")
                continue
        
        st.error("All payload formats failed")
        return {"nodes": [], "edges": []}
            
    except Exception as e:
        st.error(f"LLM call failed: {e}")
        return {"nodes": [], "edges": []}

# =======================
# üîç LangGraph Setup
# =======================
class GraphState(TypedDict):
    file: object
    text: str
    graph: dict

def extract_step(state: GraphState):
    text = extract_text_from_file(state["file"])
    st.write("**Extracted Text Length:**", len(text))  # Debug
    if text:
        st.write("**Text Sample:**", text[:200])  # Debug
    return {"text": text}

def extract_kg_step(state: GraphState):
    graph = llm_extract_graph(state["text"])
    return {"graph": graph}

def build_graph_pipeline():
    workflow = StateGraph(GraphState)
    workflow.add_node("extract_text", extract_step)
    workflow.add_node("extract_kg", extract_kg_step)
    workflow.set_entry_point("extract_text")
    workflow.add_edge("extract_text", "extract_kg")
    workflow.add_edge("extract_kg", END)
    return workflow.compile()

# =======================
# üåê Visualization
# =======================
def show_graph(graph_data):
    try:
        net = Network(height="600px", width="100%", directed=True, bgcolor="#ffffff")
        
        color_map = {"Application": "#6C5CE7", "Database": "#00B894", "Component": "#FAB1A0"}
        
        for node in graph_data.get("nodes", []):
            color = color_map.get(node.get("type", ""), "#BDC3C7")
            net.add_node(node["id"], label=node["id"], color=color, size=25)

        for edge in graph_data.get("edges", []):
            net.add_edge(edge["source"], edge["target"], label=edge["type"])

        import uuid
        temp_file = f"temp_{uuid.uuid4().hex[:8]}.html"
        temp_path = os.path.join(tempfile.gettempdir(), temp_file)
        
        net.save_graph(temp_path)
        with open(temp_path, "r", encoding="utf-8") as f:
            html = f.read()
        components.html(html, height=650)
        
        try:
            os.unlink(temp_path)
        except:
            pass
            
    except Exception as e:
        st.error(f"Visualization error: {e}")

# =======================
# üñºÔ∏è Streamlit UI
# =======================
def main():
    st.title("üîó Knowledge Graph POC (with LangGraph)")
    
    llm_status = "üü¢ Active" if LLM_USERNAME != "your_username_here" else "üî¥ Demo Mode"
    st.write(f"**LLM Status:** {llm_status}")
    
    uploaded = st.file_uploader("üìÅ Upload Document", type=["xlsx", "docx", "csv"])

    if uploaded:
        with st.spinner("üîç Processing through LangGraph pipeline..."):
            graph_pipeline = build_graph_pipeline()
            output = graph_pipeline.invoke({"file": uploaded})
            kg = output["graph"]

        if kg.get("nodes"):
            st.success("‚úÖ Knowledge graph generated!")
            show_graph(kg)
            
            if kg.get("nodes"):
                node_ids = [n['id'] for n in kg["nodes"]]
                selected_node = st.selectbox("üîç Select node", node_ids)
                
                if selected_node:
                    node_info = next((n for n in kg["nodes"] if n["id"] == selected_node), None)
                    related_edges = [e for e in kg["edges"] if e["source"] == selected_node or e["target"] == selected_node]
                    
                    st.subheader(f"üìå {selected_node}")
                    if node_info:
                        st.write(f"**Type:** {node_info.get('type', 'Unknown')}")
                    
                    for rel in related_edges:
                        direction = "‚Üí" if rel["source"] == selected_node else "‚Üê"
                        other = rel["target"] if rel["source"] == selected_node else rel["source"]
                        st.write(f"‚Ä¢ {selected_node} {direction} **{rel['type']}** {direction} {other}")
            
            st.download_button("üì• Download JSON", json.dumps(kg, indent=2), file_name="kg.json")
        else:
            st.warning("No graph data extracted.")

if __name__ == "__main__":
    main()
