# langgraph_poc.py

import streamlit as st
import pandas as pd
from docx import Document
import json
import base64
import re
import requests
from langgraph.graph import StateGraph, END
from typing import TypedDict
from pyvis.network import Network
import tempfile
import os
import streamlit.components.v1 as components

# =======================
# üîß LLM CONFIG
# =======================
LLM_API_URL = "https://your-llm-endpoint.com/v1/chat/completions"
LLM_USERNAME = "your_username_here"
LLM_PASSWORD = "your_password_here"

def get_basic_auth():
    creds = f"{LLM_USERNAME}:{LLM_PASSWORD}"
    return base64.b64encode(creds.encode()).decode()

# =======================
# üìÑ TEXT EXTRACTORS
# =======================
def extract_text_from_file(file):
    try:
        if file.name.endswith(".xlsx"):
            df = pd.read_excel(file)
            return df.to_csv(index=False)
        elif file.name.endswith(".docx"):
            doc = Document(file)
            return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        elif file.name.endswith(".csv"):
            df = pd.read_csv(file)
            return df.to_csv(index=False)
    except Exception as e:
        return f"Error reading file: {e}"
    return ""

# =======================
# ü§ñ LLM Functions
# =======================
def llm_extract_graph(text):
    if LLM_USERNAME == "your_username_here":
        return {
            "nodes": [{"id": "Web App", "type": "Application"}, {"id": "Database", "type": "Database"}],
            "edges": [{"source": "Web App", "target": "Database", "type": "USES"}]
        }
    
    prompt = f"""Extract entities and relationships from this document. Return JSON only:
{{"nodes": [{{"id": "name", "type": "Application|Database|Component"}}], "edges": [{{"source": "A", "target": "B", "type": "USES|RUNS_ON"}}]}}

Document: {text[:2000]}"""
    
    try:
        headers = {"Authorization": f"Basic {get_basic_auth()}", "Content-Type": "application/json"}
        payload = {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": prompt}], "temperature": 0.3, "max_tokens": 600}
        response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=30)
        
        if response.status_code != 200:
            st.error(f"LLM API error: {response.status_code} - {response.text}")
            return {"nodes": [], "edges": []}
            
        content = response.json()["choices"][0]["message"]["content"]
        st.write("**LLM Response:**", content)  # Debug output
        
        # Try to extract JSON
        match = re.search(r'{.*}', content, re.DOTALL)
        if match:
            try:
                result = json.loads(match.group())
                st.write("**Parsed JSON:**", result)  # Debug output
                return result
            except json.JSONDecodeError as e:
                st.error(f"JSON parsing error: {e}")
                st.write("**Raw content:**", content)
                return {"nodes": [], "edges": []}
        else:
            st.error("No JSON found in LLM response")
            st.write("**Raw content:**", content)
            return {"nodes": [], "edges": []}
            
    except Exception as e:
        st.error(f"LLM call failed: {e}")
        return {"nodes": [], "edges": []}

# =======================
# üîç LangGraph Setup
# =======================
class GraphState(TypedDict):
    file: object
    text: str
    graph: dict

def extract_step(state: GraphState):
    text = extract_text_from_file(state["file"])
    st.write("**Extracted Text Length:**", len(text))  # Debug
    if text:
        st.write("**Text Sample:**", text[:200])  # Debug
    return {"text": text}

def extract_kg_step(state: GraphState):
    graph = llm_extract_graph(state["text"])
    return {"graph": graph}

def build_graph_pipeline():
    workflow = StateGraph(GraphState)
    workflow.add_node("extract_text", extract_step)
    workflow.add_node("extract_kg", extract_kg_step)
    workflow.set_entry_point("extract_text")
    workflow.add_edge("extract_text", "extract_kg")
    workflow.add_edge("extract_kg", END)
    return workflow.compile()

# =======================
# üåê Visualization
# =======================
def show_graph(graph_data):
    try:
        net = Network(height="600px", width="100%", directed=True, bgcolor="#ffffff")
        
        color_map = {"Application": "#6C5CE7", "Database": "#00B894", "Component": "#FAB1A0"}
        
        for node in graph_data.get("nodes", []):
            color = color_map.get(node.get("type", ""), "#BDC3C7")
            net.add_node(node["id"], label=node["id"], color=color, size=25)

        for edge in graph_data.get("edges", []):
            net.add_edge(edge["source"], edge["target"], label=edge["type"])

        import uuid
        temp_file = f"temp_{uuid.uuid4().hex[:8]}.html"
        temp_path = os.path.join(tempfile.gettempdir(), temp_file)
        
        net.save_graph(temp_path)
        with open(temp_path, "r", encoding="utf-8") as f:
            html = f.read()
        components.html(html, height=650)
        
        try:
            os.unlink(temp_path)
        except:
            pass
            
    except Exception as e:
        st.error(f"Visualization error: {e}")

# =======================
# üñºÔ∏è Streamlit UI
# =======================
def main():
    st.title("üîó Knowledge Graph POC (with LangGraph)")
    
    llm_status = "üü¢ Active" if LLM_USERNAME != "your_username_here" else "üî¥ Demo Mode"
    st.write(f"**LLM Status:** {llm_status}")
    
    uploaded = st.file_uploader("üìÅ Upload Document", type=["xlsx", "docx", "csv"])

    if uploaded:
        with st.spinner("üîç Processing through LangGraph pipeline..."):
            graph_pipeline = build_graph_pipeline()
            output = graph_pipeline.invoke({"file": uploaded})
            kg = output["graph"]

        if kg.get("nodes"):
            st.success("‚úÖ Knowledge graph generated!")
            show_graph(kg)
            
            if kg.get("nodes"):
                node_ids = [n['id'] for n in kg["nodes"]]
                selected_node = st.selectbox("üîç Select node", node_ids)
                
                if selected_node:
                    node_info = next((n for n in kg["nodes"] if n["id"] == selected_node), None)
                    related_edges = [e for e in kg["edges"] if e["source"] == selected_node or e["target"] == selected_node]
                    
                    st.subheader(f"üìå {selected_node}")
                    if node_info:
                        st.write(f"**Type:** {node_info.get('type', 'Unknown')}")
                    
                    for rel in related_edges:
                        direction = "‚Üí" if rel["source"] == selected_node else "‚Üê"
                        other = rel["target"] if rel["source"] == selected_node else rel["source"]
                        st.write(f"‚Ä¢ {selected_node} {direction} **{rel['type']}** {direction} {other}")
            
            st.download_button("üì• Download JSON", json.dumps(kg, indent=2), file_name="kg.json")
        else:
            st.warning("No graph data extracted.")

if __name__ == "__main__":
    main()
