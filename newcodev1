import streamlit as st
import pandas as pd
import json
import base64
import re
import requests
from pyvis.network import Network
import tempfile
import os
import streamlit.components.v1 as components

# =======================
# üîß CONFIGURATION
# =======================
st.set_page_config(page_title="Knowledge Graph Builder", layout="wide")

# LLM Configuration
LLM_API_URL = "https://your-llm-endpoint.com/v1/chat/completions"
LLM_USERNAME = "your_username_here"  # Replace with actual username
LLM_PASSWORD = "your_password_here"  # Replace with actual password

def get_basic_auth():
    """Create basic auth header"""
    creds = f"{LLM_USERNAME}:{LLM_PASSWORD}"
    return base64.b64encode(creds.encode()).decode()

# =======================
# üìÑ DATA EXTRACTION
# =======================
def extract_data_from_excel(file):
    """Extract and structure data from Excel file"""
    try:
        df = pd.read_excel(file)
        
        # Show basic info
        st.write("**üìã File Info:**")
        st.write(f"- Rows: {len(df)}")
        st.write(f"- Columns: {len(df.columns)}")
        st.write(f"- Headers: {list(df.columns)}")
        
        # Show sample data
        st.write("**üìä Sample Data:**")
        st.dataframe(df.head())
        
        # Create structured summary for LLM
        summary = f"EXCEL DATA ANALYSIS:\n\n"
        summary += f"HEADERS: {list(df.columns)}\n\n"
        summary += f"TOTAL ROWS: {len(df)}\n\n"
        summary += f"SAMPLE DATA ROWS:\n"
        
        for i, row in df.head(10).iterrows():
            row_dict = row.to_dict()
            summary += f"Row {i+1}: {row_dict}\n"
        
        return summary
        
    except Exception as e:
        st.error(f"Error reading Excel file: {e}")
        return None

# =======================
# ü§ñ LLM KNOWLEDGE GRAPH EXTRACTION
# =======================
def test_llm_connection():
    """Test LLM with a simple request to see what format it expects"""
    st.write("**üß™ Testing LLM with simple request...**")
    
    # Simple test prompt
    test_prompt = "Hello, please respond with: SUCCESS"
    
    # Try different payload formats that Llama APIs commonly use
    payload_formats = [
        # Format 1: Standard
        {
            "inputs": test_prompt,
            "parameters": {"temperature": 0.1, "max_new_tokens": 50}
        },
        # Format 2: Messages format
        {
            "messages": [{"role": "user", "content": test_prompt}],
            "temperature": 0.1,
            "max_tokens": 50
        },
        # Format 3: Prompt field
        {
            "prompt": test_prompt,
            "temperature": 0.1,
            "max_tokens": 50
        },
        # Format 4: Text field
        {
            "text": test_prompt,
            "parameters": {"temperature": 0.1, "max_new_tokens": 50}
        }
    ]
    
    headers = {
        "Authorization": f"Basic {get_basic_auth()}", 
        "Content-Type": "application/json"
    }
    
    for i, payload in enumerate(payload_formats, 1):
        try:
            st.write(f"**Testing Format {i}:** {list(payload.keys())}")
            response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                try:
                    resp_json = response.json()
                    st.write(f"‚úÖ Format {i} - Status 200:")
                    st.json(resp_json)
                    
                    # Check if we got actual content
                    content = ""
                    if isinstance(resp_json, dict):
                        for key in ["generated_text", "output", "response", "text", "content"]:
                            if key in resp_json:
                                content = resp_json[key]
                                break
                    elif isinstance(resp_json, list) and len(resp_json) > 0:
                        for key in ["generated_text", "output", "response", "text", "content"]:
                            if key in resp_json[0]:
                                content = resp_json[0][key]
                                break
                    
                    if content and "SUCCESS" in str(content):
                        st.success(f"üéâ Format {i} WORKS! Use this format.")
                        return i, payload_formats[i-1]
                    else:
                        st.warning(f"Format {i} returned empty or unexpected content")
                        
                except json.JSONDecodeError:
                    st.error(f"Format {i} - Invalid JSON response")
                    st.code(response.text[:200])
            else:
                st.error(f"Format {i} - Status {response.status_code}: {response.text[:100]}")
                
        except Exception as e:
            st.error(f"Format {i} - Error: {e}")
    
    return None, None

def create_knowledge_graph_with_llm(data_summary):
    """Use LLM to extract knowledge graph - with format testing"""
    
    # Test LLM first
    working_format_num, working_format = test_llm_connection()
    
    if not working_format:
        st.error("‚ùå None of the common API formats worked. Check your LLM API documentation.")
        return None
    
    st.success(f"‚úÖ Using working format {working_format_num}")
    
    # Strong prompt
    prompt = f"""You are a KNOWLEDGE GRAPH EXPERT creating a proper semantic network from business data.

KNOWLEDGE GRAPH FUNDAMENTALS:
- NOUNS (Entities) = People, Systems, Technologies, Locations, etc.
- VERBS (Relationships) = Actions/connections between entities
- GOAL = Reveal HIDDEN CONNECTIONS that aren't obvious from spreadsheet rows

YOUR DATA:
{data_summary}

KNOWLEDGE GRAPH CREATION INSTRUCTIONS:

1. IDENTIFY NOUNS (ENTITIES):
   - Extract ALL system names (Risk Management System, Customer Portal, etc.)
   - Extract ALL person names (John Smith, Jane Doe, etc.)
   - Extract ALL technologies (Oracle, MySQL, Linux, Windows, etc.)
   - Extract ALL locations/environments (DataCenter-A, Production, etc.)

2. IDENTIFY VERBS (RELATIONSHIPS):
   - MANAGES (Person manages System)
   - USES (System uses Technology)
   - DEPENDS_ON (System depends on another System)
   - LOCATED_IN (System located in Location)
   - SHARES_RESOURCE (Systems share same database/server)
   - CONNECTS_TO (Systems communicate with each other)

3. DISCOVER HIDDEN CONNECTIONS:
   - If two systems have same owner ‚Üí They're RELATED
   - If two systems use same technology ‚Üí They SHARE_TECHNOLOGY
   - If systems are in same location ‚Üí They SHARE_LOCATION
   - If one system's data flows to another ‚Üí They're CONNECTED

4. KNOWLEDGE GRAPH REQUIREMENTS:
   - Use EXACT names from the data (not generic names)
   - Every entity should have 2+ connections
   - Show indirect relationships (A‚ÜíB, B‚ÜíC means A depends on C)
   - Reveal patterns that aren't obvious from individual rows

EXAMPLE PROPER KNOWLEDGE GRAPH:
{{"nodes": [
    {{"id": "Risk Management System", "type": "Application"}},
    {{"id": "John Smith", "type": "Person"}},
    {{"id": "Oracle Database", "type": "Technology"}},
    {{"id": "Customer Portal", "type": "Application"}},
    {{"id": "DataCenter-A", "type": "Location"}}
], "edges": [
    {{"source": "John Smith", "target": "Risk Management System", "type": "MANAGES"}},
    {{"source": "Risk Management System", "target": "Oracle Database", "type": "USES"}},
    {{"source": "Customer Portal", "target": "Oracle Database", "type": "USES"}},
    {{"source": "Risk Management System", "target": "Customer Portal", "type": "SHARES_RESOURCE"}},
    {{"source": "Risk Management System", "target": "DataCenter-A", "type": "LOCATED_IN"}}
]}}

RETURN ONLY THE KNOWLEDGE GRAPH JSON - NO EXPLANATIONS:"""

    # Build payload using working format
    if "inputs" in working_format:
        payload = {
            "inputs": prompt,
            "parameters": {"temperature": 0.1, "max_new_tokens": 1200}
        }
    elif "messages" in working_format:
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": 1200
        }
    elif "prompt" in working_format:
        payload = {
            "prompt": prompt,
            "temperature": 0.1,
            "max_tokens": 1200
        }
    elif "text" in working_format:
        payload = {
            "text": prompt,
            "parameters": {"temperature": 0.1, "max_new_tokens": 1200}
        }
    
    try:
        headers = {
            "Authorization": f"Basic {get_basic_auth()}", 
            "Content-Type": "application/json"
        }
        
        with st.spinner("üß† Creating knowledge graph with working format..."):
            response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            resp_json = response.json()
            
            # Extract content using the same method that worked in test
            content = ""
            if isinstance(resp_json, dict):
                for key in ["generated_text", "output", "response", "text", "content"]:
                    if key in resp_json and resp_json[key]:
                        content = resp_json[key]
                        break
            elif isinstance(resp_json, list) and len(resp_json) > 0:
                for key in ["generated_text", "output", "response", "text", "content"]:
                    if key in resp_json[0] and resp_json[0][key]:
                        content = resp_json[0][key]
                        break
            
            if not content:
                st.error("‚ùå LLM returned empty content even with working format")
                st.json(resp_json)
                return None
            
            st.write("**ü§ñ LLM Response:**")
            st.code(content[:400] + "..." if len(content) > 400 else content)
            
            # Extract JSON from response
            graph_data = extract_json_from_text(content)
            
            if graph_data and validate_graph(graph_data):
                st.success(f"‚úÖ Knowledge Graph Created: {len(graph_data['nodes'])} entities, {len(graph_data['edges'])} relationships")
                return graph_data
            else:
                st.error("‚ö†Ô∏è Could not extract valid graph from LLM response")
                return None
        else:
            st.error(f"‚ùå LLM API Error: {response.status_code} - {response.text}")
            return None
            
    except Exception as e:
        st.error(f"‚ùå Error calling LLM: {e}")
        return None

def extract_json_from_text(text):
    """Extract JSON from LLM response - improved extraction"""
    
    # First, let's see what we got
    st.write("**üîç Raw LLM Response:**")
    st.text_area("Full Response", text, height=200)
    
    # Try multiple extraction methods
    methods = [
        # Method 1: Look for JSON blocks
        lambda t: re.search(r'```json\s*(\{.*?\})\s*```', t, re.DOTALL),
        lambda t: re.search(r'```\s*(\{.*?\})\s*```', t, re.DOTALL),
        
        # Method 2: Look for any JSON with nodes/edges
        lambda t: re.search(r'(\{[^{}]*"nodes"[^{}]*"edges"[^{}]*\})', t, re.DOTALL),
        lambda t: re.search(r'(\{.*?"nodes".*?"edges".*?\})', t, re.DOTALL),
        
        # Method 3: Find JSON anywhere in text
        lambda t: re.search(r'(\{.*\})', t, re.DOTALL)
    ]
    
    for i, method in enumerate(methods):
        try:
            match = method(text)
            if match:
                json_str = match.group(1)
                st.write(f"**Method {i+1} found JSON:**")
                st.code(json_str[:500] + "..." if len(json_str) > 500 else json_str)
                
                # Try to parse it
                try:
                    # Clean up common issues
                    cleaned = json_str.strip()
                    # Remove trailing commas
                    cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
                    # Fix single quotes to double quotes
                    cleaned = re.sub(r"'([^']*)':", r'"\1":', cleaned)
                    
                    result = json.loads(cleaned)
                    
                    if validate_graph(result):
                        st.success(f"‚úÖ Valid graph found with method {i+1}")
                        return result
                    else:
                        st.warning(f"‚ö†Ô∏è Method {i+1}: JSON found but not valid graph structure")
                        
                except json.JSONDecodeError as e:
                    st.warning(f"‚ö†Ô∏è Method {i+1}: JSON parse error - {e}")
                    continue
                    
        except Exception as e:
            continue
    
    # If all methods fail, try to manually construct from text
    st.write("**üõ†Ô∏è Attempting manual extraction...**")
    
    # Look for node and edge patterns in text
    node_pattern = r'"id":\s*"([^"]+)".*?"type":\s*"([^"]+)"'
    edge_pattern = r'"source":\s*"([^"]+)".*?"target":\s*"([^"]+)".*?"type":\s*"([^"]+)"'
    
    nodes = []
    edges = []
    
    # Extract nodes
    node_matches = re.findall(node_pattern, text)
    for node_id, node_type in node_matches:
        nodes.append({"id": node_id, "type": node_type})
    
    # Extract edges  
    edge_matches = re.findall(edge_pattern, text)
    for source, target, edge_type in edge_matches:
        edges.append({"source": source, "target": target, "type": edge_type})
    
    if nodes and edges:
        manual_graph = {"nodes": nodes, "edges": edges}
        st.success(f"‚úÖ Manual extraction found {len(nodes)} nodes, {len(edges)} edges")
        return manual_graph
    
    st.error("‚ùå Could not extract any valid graph structure")
    return None

def validate_graph(graph_data):
    """Validate graph structure"""
    return (isinstance(graph_data, dict) and 
            "nodes" in graph_data and "edges" in graph_data and
            isinstance(graph_data["nodes"], list) and 
            isinstance(graph_data["edges"], list) and
            len(graph_data["nodes"]) >= 3 and
            len(graph_data["edges"]) >= 2)

# =======================
# üé® VISUALIZATION
# =======================
def create_pyvis_graph(graph_data):
    """Create interactive pyvis visualization"""
    
    net = Network(
        height="600px", 
        width="100%", 
        directed=True,
        bgcolor="#f0f0f0"
    )
    
    # Color scheme for different node types
    type_colors = {
        'Application': '#FF6B6B',
        'Database': '#4ECDC4', 
        'Server': '#45B7D1',
        'Person': '#FFA07A',
        'Location': '#98D8C8',
        'Technology': '#DDA0DD',
        'Component': '#D3D3D3'
    }
    
    # Add nodes
    for node in graph_data['nodes']:
        node_id = node['id']
        node_type = node.get('type', 'Component')
        color = type_colors.get(node_type, '#D3D3D3')
        
        net.add_node(
            node_id,
            label=node_id,
            color=color,
            title=f"Type: {node_type}",
            size=25
        )
    
    # Add edges
    for edge in graph_data['edges']:
        net.add_edge(
            edge['source'],
            edge['target'],
            label=edge['type'],
            title=f"{edge['source']} {edge['type']} {edge['target']}"
        )
    
    # Set physics options
    net.set_options("""
    var options = {
      "physics": {
        "enabled": true,
        "stabilization": {"iterations": 100}
      }
    }
    """)
    
    return net

# =======================
# üñºÔ∏è MAIN UI
# =======================
def main():
    # Header
    st.title("üîó Knowledge Graph Builder")
    st.markdown("Upload an Excel file and let AI create a knowledge graph showing hidden connections!")
    
    # LLM Status
    llm_configured = LLM_USERNAME != "your_username_here"
    status = "üü¢ Connected" if llm_configured else "üî¥ Configure LLM credentials"
    st.sidebar.markdown(f"**LLM Status:** {status}")
    
    if not llm_configured:
        st.sidebar.warning("Update LLM_USERNAME and LLM_PASSWORD in the code")
    
    # File upload
    uploaded_file = st.file_uploader(
        "üìÅ Upload Excel File", 
        type=['xlsx', 'xls'],
        help="Upload an Excel file with your data"
    )
    
    if uploaded_file:
        # Extract data
        data_summary = extract_data_from_excel(uploaded_file)
        
        if data_summary:
            # Create knowledge graph
            st.markdown("---")
            st.markdown("### üß† Creating Knowledge Graph")
            
            if llm_configured:
                graph_data = create_knowledge_graph_with_llm(data_summary)
            else:
                st.error("‚ö†Ô∏è Configure LLM credentials to create knowledge graph")
                return
            
            if not graph_data:
                st.error("‚ùå Failed to create knowledge graph")
                return
            
            # Show graph details
            st.markdown("### üìä Graph Details")
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Entities (Nouns)", len(graph_data['nodes']))
                
                # Show entity types
                entity_types = {}
                for node in graph_data['nodes']:
                    node_type = node.get('type', 'Unknown')
                    if node_type not in entity_types:
                        entity_types[node_type] = []
                    entity_types[node_type].append(node['id'])
                
                st.write("**Entity Types:**")
                for etype, entities in entity_types.items():
                    st.write(f"‚Ä¢ **{etype}:** {', '.join(entities[:3])}")
            
            with col2:
                st.metric("Relationships (Verbs)", len(graph_data['edges']))
                
                # Show relationship types
                rel_types = {}
                for edge in graph_data['edges']:
                    rel_type = edge.get('type', 'Unknown')
                    rel_types[rel_type] = rel_types.get(rel_type, 0) + 1
                
                st.write("**Relationship Types:**")
                for rtype, count in rel_types.items():
                    st.write(f"‚Ä¢ **{rtype}:** {count} connections")
            
            # Visualization
            st.markdown("### üåê Interactive Knowledge Graph")
            
            try:
                # Create pyvis graph
                net = create_pyvis_graph(graph_data)
                
                # Save to temp file and display
                import uuid
                temp_file = f"graph_{uuid.uuid4().hex[:8]}.html"
                temp_path = os.path.join(tempfile.gettempdir(), temp_file)
                
                net.save_graph(temp_path)
                
                # Read and display
                with open(temp_path, "r", encoding="utf-8") as f:
                    html_content = f.read()
                
                components.html(html_content, height=650)
                
                # Cleanup
                try:
                    os.unlink(temp_path)
                except:
                    pass
                    
            except Exception as e:
                st.error(f"Visualization error: {e}")
                st.info("üí° Make sure pyvis is installed: `pip install pyvis`")
            
            # Download option
            st.markdown("### üíæ Export")
            st.download_button(
                "üì• Download Graph JSON",
                json.dumps(graph_data, indent=2),
                file_name="knowledge_graph.json",
                mime="application/json"
            )
    
    else:
        # Instructions
        st.markdown("""
        ### üöÄ How it works:
        
        1. **Upload** your Excel file
        2. **AI analyzes** your data to find entities and relationships  
        3. **Interactive graph** shows hidden connections
        
        ### üí° What you'll discover:
        - **Entities (Nouns):** Systems, People, Technologies, Locations
        - **Relationships (Verbs):** Who manages what, what depends on what
        - **Hidden Connections:** Patterns not obvious from spreadsheet rows
        """)

if __name__ == "__main__":
    main()
