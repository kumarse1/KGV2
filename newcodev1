import streamlit as st
import pandas as pd
import json
import base64
import re
import requests
from pyvis.network import Network
import tempfile
import os
import streamlit.components.v1 as components

# =======================
# üîß CONFIGURATION
# =======================
st.set_page_config(page_title="Knowledge Graph Builder", layout="wide")

# LLM Configuration
LLM_API_URL = "https://your-llm-endpoint.com/v1/chat/completions"
LLM_USERNAME = "your_username_here"  # Replace with actual username
LLM_PASSWORD = "your_password_here"  # Replace with actual password

def get_basic_auth():
    """Create basic auth header"""
    creds = f"{LLM_USERNAME}:{LLM_PASSWORD}"
    return base64.b64encode(creds.encode()).decode()

# =======================
# üìÑ DATA EXTRACTION
# =======================
def extract_data_from_excel(file):
    """Extract and structure data from Excel file"""
    try:
        df = pd.read_excel(file)
        
        # Show basic info
        st.write("**üìã File Info:**")
        st.write(f"- Rows: {len(df)}")
        st.write(f"- Columns: {len(df.columns)}")
        st.write(f"- Headers: {list(df.columns)}")
        
        # Show sample data
        st.write("**üìä Sample Data:**")
        st.dataframe(df.head())
        
        # Create structured summary for LLM
        summary = f"EXCEL DATA ANALYSIS:\n\n"
        summary += f"HEADERS: {list(df.columns)}\n\n"
        summary += f"TOTAL ROWS: {len(df)}\n\n"
        summary += f"SAMPLE DATA ROWS:\n"
        
        for i, row in df.head(10).iterrows():
            row_dict = row.to_dict()
            summary += f"Row {i+1}: {row_dict}\n"
        
        return summary
        
    except Exception as e:
        st.error(f"Error reading Excel file: {e}")
        return None

# =======================
# ü§ñ LLM KNOWLEDGE GRAPH EXTRACTION
# =======================
def test_llm_connection():
    """Test LLM with a simple request to see what format it expects"""
    st.write("**üß™ Testing LLM with simple request...**")
    
    # Simple test prompt
    test_prompt = "Hello, please respond with: SUCCESS"
    
    # Try different payload formats that Llama APIs commonly use
    payload_formats = [
        # Format 1: Standard
        {
            "inputs": test_prompt,
            "parameters": {"temperature": 0.1, "max_new_tokens": 50}
        },
        # Format 2: Messages format
        {
            "messages": [{"role": "user", "content": test_prompt}],
            "temperature": 0.1,
            "max_tokens": 50
        },
        # Format 3: Prompt field
        {
            "prompt": test_prompt,
            "temperature": 0.1,
            "max_tokens": 50
        },
        # Format 4: Text field
        {
            "text": test_prompt,
            "parameters": {"temperature": 0.1, "max_new_tokens": 50}
        }
    ]
    
    headers = {
        "Authorization": f"Basic {get_basic_auth()}", 
        "Content-Type": "application/json"
    }
    
    for i, payload in enumerate(payload_formats, 1):
        try:
            st.write(f"**Testing Format {i}:** {list(payload.keys())}")
            response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                try:
                    resp_json = response.json()
                    st.write(f"‚úÖ Format {i} - Status 200:")
                    st.json(resp_json)
                    
                    # Check if we got actual content
                    content = ""
                    if isinstance(resp_json, dict):
                        for key in ["generated_text", "output", "response", "text", "content"]:
                            if key in resp_json:
                                content = resp_json[key]
                                break
                    elif isinstance(resp_json, list) and len(resp_json) > 0:
                        for key in ["generated_text", "output", "response", "text", "content"]:
                            if key in resp_json[0]:
                                content = resp_json[0][key]
                                break
                    
                    if content and "SUCCESS" in str(content):
                        st.success(f"üéâ Format {i} WORKS! Use this format.")
                        return i, payload_formats[i-1]
                    else:
                        st.warning(f"Format {i} returned empty or unexpected content")
                        
                except json.JSONDecodeError:
                    st.error(f"Format {i} - Invalid JSON response")
                    st.code(response.text[:200])
            else:
                st.error(f"Format {i} - Status {response.status_code}: {response.text[:100]}")
                
        except Exception as e:
            st.error(f"Format {i} - Error: {e}")
    
    return None, None

def create_knowledge_graph_with_llm(data_summary):
    """Use LLM to extract knowledge graph - with format testing and simplified prompt"""
    
    # Test LLM first
    working_format_num, working_format = test_llm_connection()
    
    if not working_format:
        st.error("‚ùå None of the common API formats worked. Check your LLM API documentation.")
        return None
    
    st.success(f"‚úÖ Using working format {working_format_num}")
    
    # SIMPLIFIED prompt that works better with Llama
    prompt = f"""Extract entities and relationships from this business data. Return JSON only.

Data:
{data_summary[:1000]}  

Create a knowledge graph with:
- Entities: People, Systems, Technologies, Locations
- Relationships: MANAGES, USES, DEPENDS_ON, LOCATED_IN

Format:
{{"nodes": [{{"id": "EntityName", "type": "EntityType"}}], "edges": [{{"source": "Source", "target": "Target", "type": "RELATIONSHIP"}}]}}

JSON:"""

    # Build payload using working format
    payload = {
        "inputs": prompt,
        "parameters": {"temperature": 0.1, "max_new_tokens": 600}  # Reduced tokens
    }
    
    try:
        headers = {
            "Authorization": f"Basic {get_basic_auth()}", 
            "Content-Type": "application/json"
        }
        
        st.write(f"**üìù Simplified prompt length:** {len(prompt)} characters")
        
        with st.spinner("üß† Creating knowledge graph with simplified prompt..."):
            response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=45)
        
        if response.status_code == 200:
            resp_json = response.json()
            
            # Extract content using the same method that worked in test
            content = ""
            if isinstance(resp_json, dict):
                for key in ["generated_text", "output", "response", "text", "content"]:
                    if key in resp_json and resp_json[key]:
                        content = resp_json[key]
                        break
            elif isinstance(resp_json, list) and len(resp_json) > 0:
                for key in ["generated_text", "output", "response", "text", "content"]:
                    if key in resp_json[0] and resp_json[0][key]:
                        content = resp_json[0][key]
                        break
            
            if not content:
                st.error("‚ùå LLM returned empty content even with simplified prompt")
                st.write("**Trying even shorter prompt...**")
                return try_minimal_prompt(data_summary)
            
            # Clean response
            if "JSON:" in content:
                content = content.split("JSON:")[-1].strip()
            
            st.write("**ü§ñ LLM Response:**")
            st.code(content[:300] + "..." if len(content) > 300 else content)
            
            # Extract JSON from response
            graph_data = extract_json_from_text(content)
            
            if graph_data and validate_graph(graph_data):
                st.success(f"‚úÖ Knowledge Graph Created: {len(graph_data['nodes'])} entities, {len(graph_data['edges'])} relationships")
                return graph_data
            else:
                st.warning("‚ö†Ô∏è Could not extract valid graph, trying minimal prompt...")
                return try_minimal_prompt(data_summary)
        else:
            st.error(f"‚ùå LLM API Error: {response.status_code} - {response.text}")
            return None
            
    except Exception as e:
        st.error(f"‚ùå Error calling LLM: {e}")
        return None

def try_minimal_prompt(data_summary):
    """Last resort: ultra-minimal prompt"""
    st.write("**üî¨ Trying ultra-minimal prompt...**")
    
    # Extract just the first few rows of data
    lines = data_summary.split('\n')
    sample_data = '\n'.join(lines[:10])  # Just first 10 lines
    
    minimal_prompt = f"""Make a simple graph from this data:

{sample_data}

Return this format:
{{"nodes":[{{"id":"Name","type":"Type"}}],"edges":[{{"source":"A","target":"B","type":"USES"}}]}}"""

    payload = {
        "inputs": minimal_prompt,
        "parameters": {"temperature": 0.1, "max_new_tokens": 300}
    }
    
    try:
        headers = {
            "Authorization": f"Basic {get_basic_auth()}", 
            "Content-Type": "application/json"
        }
        
        st.write(f"**üìù Minimal prompt length:** {len(minimal_prompt)} characters")
        
        response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            resp_json = response.json()
            
            content = ""
            if isinstance(resp_json, dict) and "generated_text" in resp_json:
                content = resp_json["generated_text"]
            elif isinstance(resp_json, list) and len(resp_json) > 0 and "generated_text" in resp_json[0]:
                content = resp_json[0]["generated_text"]
            
            if content:
                st.write("**ü§ñ Minimal Response:**")
                st.code(content)
                
                # Try to extract JSON
                graph_data = extract_json_from_text(content)
                
                if graph_data and validate_graph(graph_data):
                    st.success(f"‚úÖ Minimal Graph Created: {len(graph_data['nodes'])} entities, {len(graph_data['edges'])} relationships")
                    return graph_data
                else:
                    st.error("‚ùå Even minimal prompt failed to generate valid graph")
                    return create_basic_fallback_from_data(data_summary)
            else:
                st.error("‚ùå Minimal prompt also returned empty")
                return create_basic_fallback_from_data(data_summary)
        else:
            st.error(f"‚ùå Minimal prompt failed: {response.status_code}")
            return create_basic_fallback_from_data(data_summary)
            
    except Exception as e:
        st.error(f"‚ùå Minimal prompt error: {e}")
        return create_basic_fallback_from_data(data_summary)

def create_basic_fallback_from_data(data_summary):
    """Create a basic graph directly from the data without LLM"""
    st.write("**üõ†Ô∏è Creating basic graph from data patterns...**")
    
    nodes = []
    edges = []
    
    # Simple pattern matching from the data
    lines = data_summary.split('\n')
    
    # Look for sample data rows
    for line in lines:
        if line.startswith("Row") and ":" in line:
            try:
                row_str = line.split(": ", 1)[1]
                row_data = eval(row_str)
                
                # Extract entities from row
                for key, value in row_data.items():
                    if value and len(str(value).strip()) > 2:
                        value_clean = str(value).strip()
                        
                        # Determine entity type from column name
                        if any(word in key.lower() for word in ['system', 'application', 'app']):
                            nodes.append({"id": value_clean, "type": "Application"})
                        elif any(word in key.lower() for word in ['person', 'owner', 'manager', 'user']):
                            nodes.append({"id": value_clean, "type": "Person"})
                        elif any(word in key.lower() for word in ['database', 'db', 'data']):
                            nodes.append({"id": value_clean, "type": "Database"})
                        elif any(word in key.lower() for word in ['server', 'host', 'machine']):
                            nodes.append({"id": value_clean, "type": "Server"})
                        elif any(word in key.lower() for word in ['location', 'site', 'datacenter']):
                            nodes.append({"id": value_clean, "type": "Location"})
                        else:
                            nodes.append({"id": value_clean, "type": "Component"})
                
            except:
                continue
    
    # Remove duplicates
    unique_nodes = []
    seen_ids = set()
    for node in nodes:
        if node['id'] not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node['id'])
    
    nodes = unique_nodes[:10]  # Limit to 10 nodes
    
    # Create simple relationships
    for i in range(len(nodes) - 1):
        edges.append({
            "source": nodes[i]['id'],
            "target": nodes[i + 1]['id'],
            "type": "CONNECTS_TO"
        })
    
    if len(nodes) >= 3 and len(edges) >= 2:
        st.success(f"‚úÖ Basic graph created: {len(nodes)} entities, {len(edges)} relationships")
        return {"nodes": nodes, "edges": edges}
    else:
        st.error("‚ùå Could not create graph from data")
        return None

def extract_json_from_text(text):
    """Extract JSON from LLM response - robust JSON fixing"""
    
    st.write("**üîç Raw LLM Response for debugging:**")
    st.text_area("Full Response", text, height=150)
    
    # First try: Look for complete JSON blocks
    json_patterns = [
        r'```json\s*(\{.*?\})\s*```',
        r'```\s*(\{.*?\})\s*```',
        r'(\{[^{}]*"nodes"[^{}]*"edges"[^{}]*\})',
        r'(\{.*?"nodes".*?"edges".*?\})'
    ]
    
    for i, pattern in enumerate(json_patterns, 1):
        try:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                st.write(f"**Method {i} found JSON:**")
                st.code(match[:300] + "..." if len(match) > 300 else match)
                
                # Try parsing with aggressive cleanup
                fixed_json = fix_json_syntax(match)
                if fixed_json:
                    st.success(f"‚úÖ Method {i} succeeded after JSON fixing")
                    return fixed_json
                    
        except Exception as e:
            st.warning(f"Method {i} failed: {e}")
            continue
    
    # Manual extraction as backup
    st.write("**üõ†Ô∏è Attempting manual pattern extraction...**")
    return manual_extract_graph(text)

def fix_json_syntax(json_str):
    """Aggressively fix common JSON syntax errors"""
    try:
        # Start with the original
        fixed = json_str.strip()
        
        # Common fixes
        fixes_applied = []
        
        # 1. Remove trailing commas
        original_fixed = fixed
        fixed = re.sub(r',(\s*[}\]])', r'\1', fixed)
        if fixed != original_fixed:
            fixes_applied.append("Removed trailing commas")
        
        # 2. Fix single quotes to double quotes
        original_fixed = fixed
        fixed = re.sub(r"'([^']*)':", r'"\1":', fixed)
        fixed = re.sub(r":\s*'([^']*)'", r': "\1"', fixed)
        if fixed != original_fixed:
            fixes_applied.append("Fixed single quotes")
        
        # 3. Fix missing quotes on keys
        original_fixed = fixed
        fixed = re.sub(r'(\w+):', r'"\1":', fixed)
        if fixed != original_fixed:
            fixes_applied.append("Added quotes to keys")
        
        # 4. Fix missing quotes on string values (but not numbers/booleans)
        original_fixed = fixed
        fixed = re.sub(r':\s*([a-zA-Z][a-zA-Z0-9\s_-]*)\s*([,}\]])', r': "\1"\2', fixed)
        if fixed != original_fixed:
            fixes_applied.append("Added quotes to string values")
        
        # 5. Fix double quotes inside strings
        original_fixed = fixed
        fixed = re.sub(r'"\s*([^"]*)"([^"]*)"([^"]*)\s*"', r'"\1\2\3"', fixed)
        if fixed != original_fixed:
            fixes_applied.append("Fixed nested quotes")
        
        # 6. Ensure proper structure
        if not fixed.startswith('{'):
            fixed = '{' + fixed
        if not fixed.endswith('}'):
            fixed = fixed + '}'
        
        st.write(f"**JSON fixes applied:** {', '.join(fixes_applied) if fixes_applied else 'None needed'}")
        st.code(f"Fixed JSON:\n{fixed[:200]}...")
        
        # Try to parse
        result = json.loads(fixed)
        
        if validate_graph(result):
            return result
        else:
            st.warning("Fixed JSON parses but doesn't have valid graph structure")
            return None
            
    except json.JSONDecodeError as e:
        st.warning(f"JSON still invalid after fixes: {e}")
        return None
    except Exception as e:
        st.warning(f"Error during JSON fixing: {e}")
        return None

def manual_extract_graph(text):
    """Extract nodes and edges using regex patterns when JSON parsing fails"""
    st.write("**üîß Manual extraction from text patterns...**")
    
    nodes = []
    edges = []
    
    # Pattern 1: Look for node-like patterns
    node_patterns = [
        r'"id":\s*"([^"]+)"[^}]*"type":\s*"([^"]+)"',
        r'"id":\s*\'([^\']+)\'[^}]*"type":\s*\'([^\']+)\'',
        r'id:\s*"([^"]+)"[^}]*type:\s*"([^"]+)"',
        r'"([^"]+)"\s*,\s*"([^"]+)"'  # Simple comma-separated pairs
    ]
    
    for pattern in node_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            if len(match) == 2:
                node_id, node_type = match
                if len(node_id) > 1 and len(node_type) > 1:  # Valid looking
                    nodes.append({"id": node_id.strip(), "type": node_type.strip()})
    
    # Pattern 2: Look for edge-like patterns  
    edge_patterns = [
        r'"source":\s*"([^"]+)"[^}]*"target":\s*"([^"]+)"[^}]*"type":\s*"([^"]+)"',
        r'"source":\s*\'([^\']+)\'[^}]*"target":\s*\'([^\']+)\'[^}]*"type":\s*\'([^\']+)\'',
        r'source:\s*"([^"]+)"[^}]*target:\s*"([^"]+)"[^}]*type:\s*"([^"]+)"'
    ]
    
    for pattern in edge_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            if len(match) == 3:
                source, target, edge_type = match
                if all(len(x.strip()) > 1 for x in [source, target, edge_type]):
                    edges.append({
                        "source": source.strip(), 
                        "target": target.strip(), 
                        "type": edge_type.strip()
                    })
    
    # Remove duplicates
    unique_nodes = []
    seen_ids = set()
    for node in nodes:
        if node['id'] not in seen_ids:
            unique_nodes.append(node)
            seen_ids.add(node['id'])
    
    unique_edges = []
    seen_edges = set()
    for edge in edges:
        edge_key = (edge['source'], edge['target'], edge['type'])
        if edge_key not in seen_edges:
            unique_edges.append(edge)
            seen_edges.add(edge_key)
    
    st.write(f"**Manual extraction results:**")
    st.write(f"- Found {len(unique_nodes)} unique nodes")
    st.write(f"- Found {len(unique_edges)} unique edges")
    
    if len(unique_nodes) >= 3 and len(unique_edges) >= 1:
        result = {"nodes": unique_nodes[:10], "edges": unique_edges[:15]}  # Limit size
        st.success("‚úÖ Manual extraction successful!")
        
        # Show what was extracted
        st.write("**Extracted Nodes:**")
        for node in result['nodes'][:5]:
            st.write(f"- {node['id']} ({node['type']})")
        
        st.write("**Extracted Edges:**")  
        for edge in result['edges'][:5]:
            st.write(f"- {edge['source']} ‚Üí {edge['type']} ‚Üí {edge['target']}")
        
        return result
    else:
        st.error("‚ùå Manual extraction found insufficient data")
        st.write("Trying simple word extraction...")
        return extract_simple_entities(text)

def extract_simple_entities(text):
    """Last resort: extract simple entities from any text"""
    st.write("**üéØ Simple entity extraction as last resort...**")
    
    # Look for quoted strings that might be entities
    quoted_strings = re.findall(r'"([^"]{3,30})"', text)
    
    nodes = []
    edges = []
    
    # Create nodes from quoted strings
    for i, entity in enumerate(quoted_strings[:8]):  # Limit to 8
        if any(keyword in entity.lower() for keyword in ['system', 'app', 'portal', 'management']):
            node_type = "Application"
        elif any(keyword in entity.lower() for keyword in ['database', 'db', 'data']):
            node_type = "Database"  
        elif any(keyword in entity.lower() for keyword in ['server', 'host']):
            node_type = "Server"
        elif len(entity.split()) == 2 and entity.split()[1].lower() not in ['system', 'database']:
            node_type = "Person"  # Likely a person name
        else:
            node_type = "Component"
        
        nodes.append({"id": entity, "type": node_type})
    
    # Create simple connections
    for i in range(len(nodes) - 1):
        edges.append({
            "source": nodes[i]['id'],
            "target": nodes[i + 1]['id'],
            "type": "CONNECTS_TO"
        })
    
    if len(nodes) >= 3:
        st.success(f"‚úÖ Simple extraction created {len(nodes)} nodes, {len(edges)} edges")
        return {"nodes": nodes, "edges": edges}
    else:
        st.error("‚ùå Could not extract any meaningful entities")
        return None

def validate_graph(graph_data):
    """Validate graph structure"""
    return (isinstance(graph_data, dict) and 
            "nodes" in graph_data and "edges" in graph_data and
            isinstance(graph_data["nodes"], list) and 
            isinstance(graph_data["edges"], list) and
            len(graph_data["nodes"]) >= 3 and
            len(graph_data["edges"]) >= 2)

# =======================
# üé® VISUALIZATION
# =======================
def create_pyvis_graph(graph_data):
    """Create interactive pyvis visualization"""
    
    net = Network(
        height="600px", 
        width="100%", 
        directed=True,
        bgcolor="#f0f0f0"
    )
    
    # Color scheme for different node types
    type_colors = {
        'Application': '#FF6B6B',
        'Database': '#4ECDC4', 
        'Server': '#45B7D1',
        'Person': '#FFA07A',
        'Location': '#98D8C8',
        'Technology': '#DDA0DD',
        'Component': '#D3D3D3'
    }
    
    # Add nodes
    for node in graph_data['nodes']:
        node_id = node['id']
        node_type = node.get('type', 'Component')
        color = type_colors.get(node_type, '#D3D3D3')
        
        net.add_node(
            node_id,
            label=node_id,
            color=color,
            title=f"Type: {node_type}",
            size=25
        )
    
    # Add edges
    for edge in graph_data['edges']:
        net.add_edge(
            edge['source'],
            edge['target'],
            label=edge['type'],
            title=f"{edge['source']} {edge['type']} {edge['target']}"
        )
    
    # Set physics options
    net.set_options("""
    var options = {
      "physics": {
        "enabled": true,
        "stabilization": {"iterations": 100}
      }
    }
    """)
    
    return net

# =======================
# üñºÔ∏è MAIN UI
# =======================
def main():
    # Header
    st.title("üîó Knowledge Graph Builder")
    st.markdown("Upload an Excel file and let AI create a knowledge graph showing hidden connections!")
    
    # LLM Status
    llm_configured = LLM_USERNAME != "your_username_here"
    status = "üü¢ Connected" if llm_configured else "üî¥ Configure LLM credentials"
    st.sidebar.markdown(f"**LLM Status:** {status}")
    
    if not llm_configured:
        st.sidebar.warning("Update LLM_USERNAME and LLM_PASSWORD in the code")
    
    # File upload
    uploaded_file = st.file_uploader(
        "üìÅ Upload Excel File", 
        type=['xlsx', 'xls'],
        help="Upload an Excel file with your data"
    )
    
    if uploaded_file:
        # Extract data
        data_summary = extract_data_from_excel(uploaded_file)
        
        if data_summary:
            # Create knowledge graph
            st.markdown("---")
            st.markdown("### üß† Creating Knowledge Graph")
            
            if llm_configured:
                graph_data = create_knowledge_graph_with_llm(data_summary)
            else:
                st.error("‚ö†Ô∏è Configure LLM credentials to create knowledge graph")
                return
            
            if not graph_data:
                st.error("‚ùå Failed to create knowledge graph")
                return
            
            # Show graph details
            st.markdown("### üìä Graph Details")
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Entities (Nouns)", len(graph_data['nodes']))
                
                # Show entity types
                entity_types = {}
                for node in graph_data['nodes']:
                    node_type = node.get('type', 'Unknown')
                    if node_type not in entity_types:
                        entity_types[node_type] = []
                    entity_types[node_type].append(node['id'])
                
                st.write("**Entity Types:**")
                for etype, entities in entity_types.items():
                    st.write(f"‚Ä¢ **{etype}:** {', '.join(entities[:3])}")
            
            with col2:
                st.metric("Relationships (Verbs)", len(graph_data['edges']))
                
                # Show relationship types
                rel_types = {}
                for edge in graph_data['edges']:
                    rel_type = edge.get('type', 'Unknown')
                    rel_types[rel_type] = rel_types.get(rel_type, 0) + 1
                
                st.write("**Relationship Types:**")
                for rtype, count in rel_types.items():
                    st.write(f"‚Ä¢ **{rtype}:** {count} connections")
            
            # Visualization
            st.markdown("### üåê Interactive Knowledge Graph")
            
            try:
                # Create pyvis graph
                net = create_pyvis_graph(graph_data)
                
                # Save to temp file and display
                import uuid
                temp_file = f"graph_{uuid.uuid4().hex[:8]}.html"
                temp_path = os.path.join(tempfile.gettempdir(), temp_file)
                
                net.save_graph(temp_path)
                
                # Read and display
                with open(temp_path, "r", encoding="utf-8") as f:
                    html_content = f.read()
                
                components.html(html_content, height=650)
                
                # Cleanup
                try:
                    os.unlink(temp_path)
                except:
                    pass
                    
            except Exception as e:
                st.error(f"Visualization error: {e}")
                st.info("üí° Make sure pyvis is installed: `pip install pyvis`")
            
            # Download option
            st.markdown("### üíæ Export")
            st.download_button(
                "üì• Download Graph JSON",
                json.dumps(graph_data, indent=2),
                file_name="knowledge_graph.json",
                mime="application/json"
            )
    
    else:
        # Instructions
        st.markdown("""
        ### üöÄ How it works:
        
        1. **Upload** your Excel file
        2. **AI analyzes** your data to find entities and relationships  
        3. **Interactive graph** shows hidden connections
        
        ### üí° What you'll discover:
        - **Entities (Nouns):** Systems, People, Technologies, Locations
        - **Relationships (Verbs):** Who manages what, what depends on what
        - **Hidden Connections:** Patterns not obvious from spreadsheet rows
        """)

if __name__ == "__main__":
    main()
